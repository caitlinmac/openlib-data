{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a33d58f-5506-45f9-b798-e8a7f9cd7e0c",
   "metadata": {},
   "source": [
    "# Next steps to further Analyze OpenLibrary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1b7e7-d66a-4e6d-b5d6-7390644ebe21",
   "metadata": {},
   "source": [
    "Let's clean up the data that we queried for further analysis:\n",
    "- Null data\n",
    "- Duplicates\n",
    "- Outliers\n",
    "- Scaling Numerical features\n",
    "- Balancing\n",
    "- Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f852c-6e7b-4c4e-b91a-9703f0ff9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical Features\n",
    "\n",
    "# Numerical columns\n",
    "data_df[['first_publish_year','number_of_pages_median','ratings_average','ratings_sortable','ratings_count','readinglog_count','want_to_read_count','currently_reading_count','already_read_count']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e41ad-25fe-45e3-91d4-24541cdbfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Step 0 - Instantiate Robust Scaler\n",
    "\n",
    "rb_scaler = RobustScaler() \n",
    "\n",
    "# Step 1 - Fit the scaler to the 'number_of_pages_median'\n",
    "# to \"learn\" the median value and the IQR\n",
    "\n",
    "rb_scaler.fit(pop_df[['number_of_pages_median']])\n",
    "\n",
    "# Step 2 - Scale / Transform\n",
    "# to apply the transformation (value - median) / IQR for every house\n",
    "\n",
    "#data['GrLivArea'] = rb_scaler.transform(data[['GrLivArea']]) \n",
    "pop_df['number_of_pages_median'] = rb_scaler.transform(pop_df[['number_of_pages_median']])\n",
    "\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffc4b3-ac5b-448b-a992-af06fe052198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich language column to areduce amnt of NaN\n",
    "\n",
    "# current list of all languages:\n",
    "\n",
    "unique_langs=['eng']\n",
    "#cycle through df and add any strings that are not already in the final list\n",
    "def check_lang(row):\n",
    "    if isinstance(row['language'], list):\n",
    "        for lang in row['language']:\n",
    "            if lang not in unique_langs:\n",
    "                unique_langs.append(lang)\n",
    "# for each row\n",
    "data_df.apply(check_lang, axis=1)\n",
    "\n",
    "print(unique_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec9d85-66e1-4d7f-b4ac-3bd3c2e77858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30f518-e935-4931-b457-fe77b3c32111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing languages:\n",
    "import langid\n",
    "\n",
    "# langid provides language in  ISO 639-1 whereas OpenLib appears to provide language in ISO 639-3 format\n",
    "# Here is an ISO 639-1 to ISO 639-3 Mapping Dictionary:\n",
    "iso639_1_to_3 = {\n",
    "    'af': 'afr',  # Afrikaans\n",
    "    'sq': 'sqi',  # Albanian\n",
    "    'ar': 'ara',  # Arabic\n",
    "    'hy': 'hye',  # Armenian\n",
    "    'bn': 'ben',  # Bengali\n",
    "    'bs': 'bos',  # Bosnian\n",
    "    'ca': 'cat',  # Catalan\n",
    "    'hr': 'hrv',  # Croatian\n",
    "    'cs': 'ces',  # Czech\n",
    "    'da': 'dan',  # Danish\n",
    "    'nl': 'nld',  # Dutch\n",
    "    'en': 'eng',  # English\n",
    "    'eo': 'epo',  # Esperanto\n",
    "    'et': 'est',  # Estonian\n",
    "    'fi': 'fin',  # Finnish\n",
    "    'fr': 'fra',  # French\n",
    "    'de': 'deu',  # German\n",
    "    'el': 'ell',  # Greek\n",
    "    'gu': 'guj',  # Gujarati\n",
    "    'he': 'heb',  # Hebrew\n",
    "    'hi': 'hin',  # Hindi\n",
    "    'hu': 'hun',  # Hungarian\n",
    "    'is': 'isl',  # Icelandic\n",
    "    'id': 'ind',  # Indonesian\n",
    "    'it': 'ita',  # Italian\n",
    "    'ja': 'jpn',  # Japanese\n",
    "    'jw': 'jav',  # Javanese\n",
    "    'kn': 'kan',  # Kannada\n",
    "    'km': 'khm',  # Khmer\n",
    "    'ko': 'kor',  # Korean\n",
    "    'la': 'lat',  # Latin\n",
    "    'lv': 'lav',  # Latvian\n",
    "    'lt': 'lit',  # Lithuanian\n",
    "    'mk': 'mkd',  # Macedonian\n",
    "    'ml': 'mal',  # Malayalam\n",
    "    'mn': 'mon',  # Mongolian\n",
    "    'mr': 'mar',  # Marathi\n",
    "    'my': 'mya',  # Burmese\n",
    "    'ne': 'nep',  # Nepali\n",
    "    'no': 'nor',  # Norwegian\n",
    "    'or': 'ori',  # Odia\n",
    "    'pa': 'pan',  # Punjabi\n",
    "    'pl': 'pol',  # Polish\n",
    "    'ps': 'pus',  # Pashto\n",
    "    'pt': 'por',  # Portuguese\n",
    "    'pa': 'pan',  # Punjabi\n",
    "    'ro': 'ron',  # Romanian\n",
    "    'ru': 'rus',  # Russian\n",
    "    'sa': 'san',  # Sanskrit\n",
    "    'sd': 'snd',  # Sindhi\n",
    "    'si': 'sin',  # Sinhala\n",
    "    'sk': 'slk',  # Slovak\n",
    "    'sl': 'slv',  # Slovenian\n",
    "    'es': 'spa',  # Spanish\n",
    "    'su': 'sun',  # Sundanese\n",
    "    'sw': 'swa',  # Swahili\n",
    "    'sv': 'swe',  # Swedish\n",
    "    'ta': 'tam',  # Tamil\n",
    "    'te': 'tel',  # Telugu\n",
    "    'th': 'tha',  # Thai\n",
    "    'tr': 'tur',  # Turkish\n",
    "    'uk': 'ukr',  # Ukrainian\n",
    "    'ur': 'udm',  # Urdu\n",
    "    'vi': 'vie',  # Vietnamese\n",
    "    'cy': 'cym',  # Welsh\n",
    "    'xh': 'xho',  # Xhosa\n",
    "    'yi': 'yid',  # Yiddish\n",
    "    'yo': 'yor',  # Yoruba\n",
    "    'zu': 'zul'   # Zulu\n",
    "}\n",
    "\n",
    "# Function to Identify and Convert Language Codes\n",
    "def convert_language_code(code):\n",
    "    # Check if the code is in ISO 639-1\n",
    "    if len(code) == 2 and code.isalpha():\n",
    "        # Convert ISO 639-1 to ISO 639-3\n",
    "        return iso639_1_to_3.get(code, code)\n",
    "    elif len(code) == 3 and code.isalpha():\n",
    "        # It's already in ISO 639-3\n",
    "        return code\n",
    "    else:\n",
    "        # Invalid or unknown code\n",
    "        return None\n",
    "\n",
    "# Function to identify and add language for NaN values under 'language'\n",
    "def add_lang(row):\n",
    "    # For all columns with NaN\n",
    "    if (not (isinstance(row['language'], list))) and pd.isna(row['language']):\n",
    "        #print(row['language'])\n",
    "        # Check language of title\n",
    "        newlang, _ = langid.classify(row['title'])\n",
    "        #print(f\"{row['title']} is in {newlang}\")\n",
    "        # Convert language into ISO 639-3\n",
    "        insert_lang = convert_language_code(newlang)\n",
    "        #print(f\"{newlang} is now {insert_lang}\")\n",
    "        # Insert language into list\n",
    "        row['language'] = [insert_lang]\n",
    "        # Add to unique languages list\n",
    "        #check_lang(row['language'])\n",
    "    return row\n",
    "\n",
    "data_df = data_df.apply(add_lang, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8955de-5ee3-4779-bf60-04212beed799",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d610-de0c-412e-a312-e3be1fc0adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUBJECT ANALYSIS USING TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656461c7-91e7-4bfd-8b9e-cb45c06ebdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataFrame\n",
    "subject_df = data_df\n",
    "subject_df = subject_df.dropna(subset=['subject'])\n",
    "\n",
    "print(subject_df['subject'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67458f6c-bedd-4523-b3fc-2e216d738525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert keywords to a format suitable for analysis\n",
    "# Assuming 'subject' column contains lists of keywords\n",
    "subject_df['subject'] = subject_df['subject'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(subject_df['subject'])\n",
    "y = subject_df['readinglog_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f28c91-bc80-460d-bbe8-912ed93e2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f'R^2 score: {model.score(X_test, y_test)}')\n",
    "\n",
    "# Visualization of keyword importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importance = model.coef_\n",
    "keywords_importance = pd.DataFrame({'Keyword': feature_names, 'Importance': importance})\n",
    "\n",
    "sns.barplot(x='Importance', y='Keyword', data=keywords_importance.sort_values(by='Importance', ascending=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a8fa7-f62a-46fd-9704-ea7a29843dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fetch_openlibrary_data(api_url, params=None, max_retries=5, sleep_between_retries=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "            response.raise_for_status()  # Raise an error on bad status codes\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error occurred: {e}\")\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Connection error occurred: {e}\")\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            print(f\"Timeout occurred: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"General error occurred: {e}\")\n",
    "        \n",
    "        retries += 1\n",
    "        print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "        time.sleep(sleep_between_retries)\n",
    "\n",
    "    return None\n",
    "\n",
    "def fetch_all_data(base_url, total_records, records_per_page=100):\n",
    "    all_data = []\n",
    "    for start in range(0, total_records, records_per_page):\n",
    "        params = {\n",
    "            'offset': start,\n",
    "            'limit': records_per_page\n",
    "        }\n",
    "        data = fetch_openlibrary_data(base_url, params=params)\n",
    "        if data:\n",
    "            all_data.extend(data['entries'])  # Adjust this based on the API response structure\n",
    "            print(f\"Retrieved {len(data['entries'])} records, Total: {len(all_data)}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve data. Exiting.\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(1)  # Throttle requests to avoid rate limiting\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Base API URL and query parameters\n",
    "api_base_url = \"https://openlibrary.org/search.json\"\n",
    "total_records = 200000  # Replace with your actual total record count\n",
    "records_per_page = 100  # Adjust based on the API's maximum limit\n",
    "\n",
    "# Fetch data\n",
    "data = fetch_all_data(api_base_url, total_records, records_per_page)\n",
    "\n",
    "# Save to a file\n",
    "with open('openlibrary_data.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "print(f\"Total records fetched: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ae9f5-5d8e-4c90-ab0a-0f5e171ac67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPRESSING:\n",
    "data_df[\"first_publish_year\"] = data_df[\"first_publish_year\"].astype(\"int16\")\n",
    "\"\"\"\n",
    "    \"number_of_pages_median\" : \"float32\",\n",
    "    \"ratings_average\" : \"float32\",\n",
    "    \"ratings_sortable\": \"float32\",\n",
    "    \"ratings_count\": \"int32\",\n",
    "    \"readinglog_count\": \"int32\",\n",
    "    \"want_to_read_count\": \"int32\",\n",
    "    \"currently_reading_count\": \"int32\",\n",
    "    \"already_read_count\": \"int32\"\n",
    "}\n",
    "\"\"\"\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182779a-0969-4c7f-8daa-753fb1375df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_string(sentence):\n",
    "    \"\"\"\n",
    "    Arg is a string\n",
    "    Perform cleaning functions to standardize string format and text for further analysis\n",
    "    \"\"\"\n",
    "    if isinstance(sentence, (list, str)):\n",
    "        # Basic cleaning\n",
    "        sentence = sentence.strip() ## remove whitespaces\n",
    "        sentence = sentence.lower() ## lowercase\n",
    "        sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "        # Advanced cleaning\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "        tokenized_sentence = word_tokenize(sentence) ## tokenize\n",
    "        stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "        tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "            w for w in tokenized_sentence if not w in stop_words\n",
    "        ]\n",
    "    \n",
    "        lemmatized = [\n",
    "            WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "            for word in tokenized_sentence_cleaned\n",
    "        ]\n",
    "    \n",
    "        cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "        return cleaned_sentence\n",
    "    else:\n",
    "        print(f\"{sentence} is not a list, nor a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f6073-e2b7-4904-9123-a38732ea3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column 'subject_clean' and put the result of our clean_string function into this column\n",
    "data_df['subject_clean'] = data_df['subject'].apply(lambda lst: [clean_string(s) for s in lst] if isinstance(lst, list) else lst)\n",
    "\n",
    "print(data_df['subject_clean'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747df30-3218-478e-ac49-676f64edd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of widely recognized genres\n",
    "book_genres = [\n",
    "    \"Literary Fiction\",\n",
    "    \"Historical Fiction\",\n",
    "    \"Science Fiction\",\n",
    "    \"Fantasy\",\n",
    "    \"Mystery\",\n",
    "    \"Thriller\",\n",
    "    \"Romance\",\n",
    "    \"Horror\",\n",
    "    \"Young Adult\",\n",
    "    \"Dystopian\",\n",
    "    \"Adventure\",\n",
    "    \"Crime\",\n",
    "    \"Magical Realism\",\n",
    "    \"Graphic Novel\",\n",
    "    \"Comic\",\n",
    "    \"Biography\",\n",
    "    \"Autobiography\",\n",
    "    \"Memoir\",\n",
    "    \"Self Help\",\n",
    "    \"True Crime\",\n",
    "    \"History\",\n",
    "    \"Travel\",\n",
    "    \"Science\",\n",
    "    \"Philosophy\",\n",
    "    \"Religion\",\n",
    "    \"Spirituality\",\n",
    "    \"Business\",\n",
    "    \"Economics\",\n",
    "    \"Health\",\n",
    "    \"Fitness\",\n",
    "    \"Politics\",\n",
    "    \"Essays\",\n",
    "    \"Cookbook\",\n",
    "    \"Art\",\n",
    "    \"Photography\",\n",
    "    \"Poetry\",\n",
    "    \"Drama\",\n",
    "    \"Play\",\n",
    "    \"Short Story\",\n",
    "    \"Children\",\n",
    "    \"New Adult\",\n",
    "    \"Chick Lit\",\n",
    "    \"Westerns\",\n",
    "    \"Classics\"\n",
    "]\n",
    "\n",
    "# Function to check if a string matches any genre in the list\n",
    "def is_genre(string):\n",
    "    for genre in book_genres:\n",
    "        if string and string.lower() == genre.lower():\n",
    "            return string.lower()\n",
    "    return None\n",
    "\n",
    "# Function to clean and extract genres from the subject_clean column\n",
    "def cleaningr(lst):\n",
    "    matched_genres = []\n",
    "    for s in lst:\n",
    "        genre = is_genre(s)\n",
    "        if genre:\n",
    "            matched_genres.append(genre)\n",
    "    return matched_genres\n",
    "\n",
    "# Apply the function to each row in the subject_clean column\n",
    "data_df['genre'] = data_df['subject_clean'].apply(lambda lst: cleaningr(lst) if isinstance(lst, list) else [])\n",
    "\n",
    "print(data_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
