{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a33d58f-5506-45f9-b798-e8a7f9cd7e0c",
   "metadata": {},
   "source": [
    "# Next steps to further Analyze OpenLibrary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1b7e7-d66a-4e6d-b5d6-7390644ebe21",
   "metadata": {},
   "source": [
    "Let's clean up the data that we queried for further analysis:\n",
    "- Null data\n",
    "- Duplicates\n",
    "- Outliers\n",
    "- Scaling Numerical features\n",
    "- Balancing\n",
    "- Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f852c-6e7b-4c4e-b91a-9703f0ff9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical Features\n",
    "\n",
    "# Numerical columns\n",
    "data_df[['first_publish_year','number_of_pages_median','ratings_average','ratings_sortable','ratings_count','readinglog_count','want_to_read_count','currently_reading_count','already_read_count']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e41ad-25fe-45e3-91d4-24541cdbfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Step 0 - Instantiate Robust Scaler\n",
    "\n",
    "rb_scaler = RobustScaler() \n",
    "\n",
    "# Step 1 - Fit the scaler to the 'number_of_pages_median'\n",
    "# to \"learn\" the median value and the IQR\n",
    "\n",
    "rb_scaler.fit(pop_df[['number_of_pages_median']])\n",
    "\n",
    "# Step 2 - Scale / Transform\n",
    "# to apply the transformation (value - median) / IQR for every house\n",
    "\n",
    "#data['GrLivArea'] = rb_scaler.transform(data[['GrLivArea']]) \n",
    "pop_df['number_of_pages_median'] = rb_scaler.transform(pop_df[['number_of_pages_median']])\n",
    "\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffc4b3-ac5b-448b-a992-af06fe052198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich language column to areduce amnt of NaN\n",
    "\n",
    "# current list of all languages:\n",
    "\n",
    "unique_langs=['eng']\n",
    "#cycle through df and add any strings that are not already in the final list\n",
    "def check_lang(row):\n",
    "    if isinstance(row['language'], list):\n",
    "        for lang in row['language']:\n",
    "            if lang not in unique_langs:\n",
    "                unique_langs.append(lang)\n",
    "# for each row\n",
    "data_df.apply(check_lang, axis=1)\n",
    "\n",
    "print(unique_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec9d85-66e1-4d7f-b4ac-3bd3c2e77858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30f518-e935-4931-b457-fe77b3c32111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing languages:\n",
    "import langid\n",
    "\n",
    "# langid provides language in  ISO 639-1 whereas OpenLib appears to provide language in ISO 639-3 format\n",
    "# Here is an ISO 639-1 to ISO 639-3 Mapping Dictionary:\n",
    "iso639_1_to_3 = {\n",
    "    'af': 'afr',  # Afrikaans\n",
    "    'sq': 'sqi',  # Albanian\n",
    "    'ar': 'ara',  # Arabic\n",
    "    'hy': 'hye',  # Armenian\n",
    "    'bn': 'ben',  # Bengali\n",
    "    'bs': 'bos',  # Bosnian\n",
    "    'ca': 'cat',  # Catalan\n",
    "    'hr': 'hrv',  # Croatian\n",
    "    'cs': 'ces',  # Czech\n",
    "    'da': 'dan',  # Danish\n",
    "    'nl': 'nld',  # Dutch\n",
    "    'en': 'eng',  # English\n",
    "    'eo': 'epo',  # Esperanto\n",
    "    'et': 'est',  # Estonian\n",
    "    'fi': 'fin',  # Finnish\n",
    "    'fr': 'fra',  # French\n",
    "    'de': 'deu',  # German\n",
    "    'el': 'ell',  # Greek\n",
    "    'gu': 'guj',  # Gujarati\n",
    "    'he': 'heb',  # Hebrew\n",
    "    'hi': 'hin',  # Hindi\n",
    "    'hu': 'hun',  # Hungarian\n",
    "    'is': 'isl',  # Icelandic\n",
    "    'id': 'ind',  # Indonesian\n",
    "    'it': 'ita',  # Italian\n",
    "    'ja': 'jpn',  # Japanese\n",
    "    'jw': 'jav',  # Javanese\n",
    "    'kn': 'kan',  # Kannada\n",
    "    'km': 'khm',  # Khmer\n",
    "    'ko': 'kor',  # Korean\n",
    "    'la': 'lat',  # Latin\n",
    "    'lv': 'lav',  # Latvian\n",
    "    'lt': 'lit',  # Lithuanian\n",
    "    'mk': 'mkd',  # Macedonian\n",
    "    'ml': 'mal',  # Malayalam\n",
    "    'mn': 'mon',  # Mongolian\n",
    "    'mr': 'mar',  # Marathi\n",
    "    'my': 'mya',  # Burmese\n",
    "    'ne': 'nep',  # Nepali\n",
    "    'no': 'nor',  # Norwegian\n",
    "    'or': 'ori',  # Odia\n",
    "    'pa': 'pan',  # Punjabi\n",
    "    'pl': 'pol',  # Polish\n",
    "    'ps': 'pus',  # Pashto\n",
    "    'pt': 'por',  # Portuguese\n",
    "    'pa': 'pan',  # Punjabi\n",
    "    'ro': 'ron',  # Romanian\n",
    "    'ru': 'rus',  # Russian\n",
    "    'sa': 'san',  # Sanskrit\n",
    "    'sd': 'snd',  # Sindhi\n",
    "    'si': 'sin',  # Sinhala\n",
    "    'sk': 'slk',  # Slovak\n",
    "    'sl': 'slv',  # Slovenian\n",
    "    'es': 'spa',  # Spanish\n",
    "    'su': 'sun',  # Sundanese\n",
    "    'sw': 'swa',  # Swahili\n",
    "    'sv': 'swe',  # Swedish\n",
    "    'ta': 'tam',  # Tamil\n",
    "    'te': 'tel',  # Telugu\n",
    "    'th': 'tha',  # Thai\n",
    "    'tr': 'tur',  # Turkish\n",
    "    'uk': 'ukr',  # Ukrainian\n",
    "    'ur': 'udm',  # Urdu\n",
    "    'vi': 'vie',  # Vietnamese\n",
    "    'cy': 'cym',  # Welsh\n",
    "    'xh': 'xho',  # Xhosa\n",
    "    'yi': 'yid',  # Yiddish\n",
    "    'yo': 'yor',  # Yoruba\n",
    "    'zu': 'zul'   # Zulu\n",
    "}\n",
    "\n",
    "# Function to Identify and Convert Language Codes\n",
    "def convert_language_code(code):\n",
    "    # Check if the code is in ISO 639-1\n",
    "    if len(code) == 2 and code.isalpha():\n",
    "        # Convert ISO 639-1 to ISO 639-3\n",
    "        return iso639_1_to_3.get(code, code)\n",
    "    elif len(code) == 3 and code.isalpha():\n",
    "        # It's already in ISO 639-3\n",
    "        return code\n",
    "    else:\n",
    "        # Invalid or unknown code\n",
    "        return None\n",
    "\n",
    "# Function to identify and add language for NaN values under 'language'\n",
    "def add_lang(row):\n",
    "    # For all columns with NaN\n",
    "    if (not (isinstance(row['language'], list))) and pd.isna(row['language']):\n",
    "        #print(row['language'])\n",
    "        # Check language of title\n",
    "        newlang, _ = langid.classify(row['title'])\n",
    "        #print(f\"{row['title']} is in {newlang}\")\n",
    "        # Convert language into ISO 639-3\n",
    "        insert_lang = convert_language_code(newlang)\n",
    "        #print(f\"{newlang} is now {insert_lang}\")\n",
    "        # Insert language into list\n",
    "        row['language'] = [insert_lang]\n",
    "        # Add to unique languages list\n",
    "        #check_lang(row['language'])\n",
    "    return row\n",
    "\n",
    "data_df = data_df.apply(add_lang, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8955de-5ee3-4779-bf60-04212beed799",
   "metadata": {},
   "source": [
    "# SUBJECT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d610-de0c-412e-a312-e3be1fc0adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUBJECT ANALYSIS USING TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656461c7-91e7-4bfd-8b9e-cb45c06ebdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataFrame\n",
    "subject_df = data_df\n",
    "subject_df = subject_df.dropna(subset=['subject'])\n",
    "\n",
    "print(subject_df['subject'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67458f6c-bedd-4523-b3fc-2e216d738525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert keywords to a format suitable for analysis\n",
    "# Assuming 'subject' column contains lists of keywords\n",
    "subject_df['subject'] = subject_df['subject'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(subject_df['subject'])\n",
    "y = subject_df['readinglog_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f28c91-bc80-460d-bbe8-912ed93e2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f'R^2 score: {model.score(X_test, y_test)}')\n",
    "\n",
    "# Visualization of keyword importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importance = model.coef_\n",
    "keywords_importance = pd.DataFrame({'Keyword': feature_names, 'Importance': importance})\n",
    "\n",
    "sns.barplot(x='Importance', y='Keyword', data=keywords_importance.sort_values(by='Importance', ascending=False))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
